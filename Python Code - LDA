import pandas as pd
import torch
from transformers import GPT2Tokenizer, GPT2Model
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import pyLDAvis
import pyLDAvis.sklearn
import nltk
from nltk.corpus import stopwords

# Load GPT-2 pre-trained model and tokenizer
model_name = 'gpt2'
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

# Load the data from CSV file
data = pd.read_csv('data.csv') # Replace 'data.csv' with your actual file path

# Extract the text data from the CSV file
documents = data['text'].tolist()

# Define custom stop words
custom_stopwords = ['insider', 'insiders', 'taxonomies','taxonomy','ve']  # Add your custom stop words here

# Combine custom stop words with NLTK stopwords
stopwords_list = stopwords.words('english') + custom_stopwords

# Tokenize and encode the documents using GPT-2 tokenizer
encoded_inputs = []
max_length = 0
for doc in documents:
    encoded_input = tokenizer.encode(doc, truncation=True, max_length=512, return_tensors='pt')[0]
    encoded_inputs.append(encoded_input)
    max_length = max(max_length, len(encoded_input))

# Pad the sequences to the same length
padded_inputs = []
for input in encoded_inputs:
    padded_input = torch.nn.functional.pad(input, (0, max_length - input.shape[0]), value=0)
    padded_inputs.append(padded_input)

padded_inputs = torch.stack(padded_inputs)

# Extract the document embeddings from GPT-2 model
model = GPT2Model.from_pretrained(model_name)
with torch.no_grad():
    model_outputs = model(input_ids=padded_inputs)
    embeddings = model_outputs.last_hidden_state[:, 0, :].numpy()  # Use the representation of the [CLS] token

# Apply topic modeling using Latent Dirichlet Allocation (LDA)
vectorizer = CountVectorizer(max_features=1000, lowercase=True, stop_words=stopwords_list)
X = vectorizer.fit_transform(documents)
feature_names = vectorizer.get_feature_names()

# Set the number of topics for LDA
n_topics = 8

# Fit LDA model to the document-term matrix
lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda_model.fit(X)

# Visualize the LDA model
lda_vis_data = pyLDAvis.sklearn.prepare(lda_model, X, vectorizer)
pyLDAvis.display(lda_vis_data)
